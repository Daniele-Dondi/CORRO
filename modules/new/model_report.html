
<html>
<head><title>Model Interpretation Report</title></head>
<body>
<h1>Model Interpretation Report</h1>
<h1>Model fitting</h1><br>Data are fitted with a <strong>Random Forest</strong> Algorithm. <br>Values of fitting are displayed below:<br>R²:0.8822998455258249<br>MAE :7.00274265828837<br>RMSE :8.84022117670664<br>Cross-validated R² scores:<br>[-0.0777129   0.4692786  -0.0418459  -0.39287957 -0.06866764]<br>Mean R² :-0.022365483284776853<br><h3>Residuals</h3>The residual graph should appear like random distributed points<br><img src="RF_residues.png" width="600"><br><br>Below the parameters sorted from the most important to the less important<br><img src="RF_feature_importance.png" width="600"><br><br><h2>Partial Dependence Plots (Main Effects)</h2>
<h3>Features</h3>
<p>This plot shows how the feature affects the model's prediction on average, keeping all other features constant.</p>
<img src="pdp_feature.png" width="600"><br>

<h2>Partial Dependence Plots (All Feature Interactions)</h2>
<p>This grid shows how pairs of features interact to influence the model's predictions. Look for curved surfaces or ridges to spot nonlinear effects and dependencies.</p>
<img src="pdp_pairs.png" width="800"><br>
<h2>SHAP Summary Plot</h2>
<h3>Shap</h3>
<p>
1. Y-axis: Feature Names<br>
Ranked by importance (top = most influential).<br>
Importance is based on the average absolute SHAP value = how much each feature contributes to predictions overall.<br>
2. X-axis: SHAP Value<br>
Represents the impact of the feature on the model’s output.<br>
Positive SHAP value -> pushes prediction higher.<br>
Negative SHAP value -> pushes prediction lower.<br>
3. Color: Feature Value<br>
Each dot is a sample.<br>
Color shows the actual value of the feature for that sample:<br>
Red = high feature value<br>
Blue = low feature value</p>
<img src="shap_summary.png" width="600"><br>
<h2>SHAP Dependence Plots</h2>
<h3>Dependence</h3>
<p>This SHAP dependence plot shows how the value of a <strong>feature</strong> impacts the prediction for each sample. Color indicates the value of interacting features, revealing potential nonlinear or interaction effects.</p>
<img src="shap_dependence.png" width="600"><br>
</body></html>