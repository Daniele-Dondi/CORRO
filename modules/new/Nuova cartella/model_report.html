
    <html>
    <head><title>Model Interpretation Report</title></head>
    <body>
    <h1>Model Interpretation Report</h1>
    <h2>Processed filename: grunwald2log.csv</h2><h1>Model fitting</h1><br>Data are fitted with a <strong>Random Forest</strong> Algorithm. <br>Values of fitting are displayed below:<br>R<sup>2</sup>:0.9773689039310846<br>MAE :0.06730145306799178<br>RMSE :0.11786549095684251<br>Cross-validated R<sup>2</sup> scores:<br>[0.82281945 0.81250005 0.81248501 0.80495577 0.83838301]<br>Mean R<sup>2</sup> :0.8182286592758728<br><h3>Residuals</h3>The residual graph should appear like random distributed points<br><img src="RF_residues.png" width="600"><br><br>Below the parameters sorted from the most important to the less important<br><img src="RF_feature_importance.png" width="600"><br><br><h2>Partial Dependence Plots (Main Effects)</h2>
    <h3>Features</h3>
    <p>This plot shows how the feature affects the model's prediction on average, keeping all other features constant.</p>
    <img src="pdp_feature.png" width="600"><br>
    
    <h2>Partial Dependence Plots (All Feature Interactions)</h2>
    <p>This grid shows how pairs of features interact to influence the model's predictions. Look for curved surfaces or ridges to spot nonlinear effects and dependencies.</p>
    <img src="pdp_pairs.png" width="800"><br>
    <h2>SHAP Summary Plot</h2>
    <h3>Shap</h3>
    <p>
    1. Y-axis: Feature Names<br>
    Ranked by importance (top = most influential).<br>
    Importance is based on the average absolute SHAP value = how much each feature contributes to predictions overall.<br>
    2. X-axis: SHAP Value<br>
    Represents the impact of the feature on the model’s output.<br>
    Positive SHAP value -> pushes prediction higher.<br>
    Negative SHAP value -> pushes prediction lower.<br>
    3. Color: Feature Value<br>
    Each dot is a sample.<br>
    Color shows the actual value of the feature for that sample:<br>
    Red = high feature value<br>
    Blue = low feature value</p>
    <img src="shap_summary.png" width="600"><br>
    <h2>SHAP Dependence Plots</h2>
    <h3>Dependence</h3>
    <p>This SHAP dependence plot shows how the value of a <strong>feature</strong> impacts the prediction for each sample. Color indicates the value of interacting features, revealing potential nonlinear or interaction effects.</p>
    <img src="shap_dependence.png" width="600"><br>
    </body></html>